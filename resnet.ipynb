{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfF3D_HQMbIo",
        "colab_type": "text"
      },
      "source": [
        "#Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah-A6aIVF_EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if train_on_gpu else \"cpu\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNrioFs9NBZh",
        "colab_type": "text"
      },
      "source": [
        "#Download the training data from cifar-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho8MOyVlNIsi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9add00ab-8a90-405e-949d-160b40a952bd"
      },
      "source": [
        "# Number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "\n",
        "# How many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# Percentage of training set to use as validationã€‚\n",
        "n_valid = 0.2\n",
        "\n",
        "# Convert data to a normalized torch.FloatTensor\n",
        "norm_mean = [0.485, 0.456, 0.406]\n",
        "norm_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "# Select training_set and testing_set\n",
        "train_data = datasets.CIFAR10(\"data\",train= True,download=True,transform = train_transform)\n",
        "\n",
        "test_data = datasets.CIFAR10(\"data\",train= False,download=True,transform = valid_transform)\n",
        "\n",
        "# Get indices for training_set and validation_set\n",
        "n_train = len(train_data)\n",
        "indices = list(range(n_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(n_valid * n_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# Define samplers for obtaining training and validation\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# Prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                       sampler = train_sampler,\n",
        "                       num_workers = num_workers)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size,\n",
        "                       sampler = valid_sampler,\n",
        "                       num_workers = num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, \n",
        "                       batch_size = batch_size,\n",
        "                       num_workers = num_workers)\n",
        "\n",
        "# Specify the image classes\n",
        "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
        "          \"horse\", \"ship\", \"truck\"]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL72RYfNQuAB",
        "colab_type": "text"
      },
      "source": [
        "#Define the Resnet acrh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFGZxpGCTJaf",
        "colab_type": "text"
      },
      "source": [
        "##step 1 define the basic block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDhKIMDxZdkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LambdaLayer(nn.Module):\n",
        "  def __init__(self, lambd):\n",
        "      super(LambdaLayer, self).__init__()\n",
        "      self.lambd = lambd\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.lambd(x)\n",
        "      \n",
        "def _weights_init(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "    init.kaiming_normal_(m.weight)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNGVlCMlRApC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
        "                           stride=stride, padding=1, \n",
        "                           bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, \n",
        "                           stride=1, padding=1, \n",
        "                           bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    ###define the downsample method for shortcut\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_planes != planes:\n",
        "      ###For CIFAR10 ResNet paper uses option A\n",
        "      self.shortcut = LambdaLayer(lambda x:\n",
        "                      F.pad(x[:, :, ::2, ::2], \n",
        "                      (0, 0, 0, 0, planes//4, \n",
        "                       planes//4), \"constant\", 0))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out += self.shortcut(x)\n",
        "    out = self.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwZ5hcCCVQuG",
        "colab_type": "text"
      },
      "source": [
        "##step2 define the resnet \n",
        "in paper "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd5DBHzJVYPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 64\n",
        "\n",
        "    #7,7,64,stride=2 output 112\n",
        "    self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(self.in_planes)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    #output 56,28,14,7\n",
        "    self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "    self.layer4 = self._make_layer(block, 512, num_blocks[2], stride=2)\n",
        "\n",
        "    #fc layer output 1*1\n",
        "    self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    self.apply(_weights_init)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    #downsampling in first block\n",
        "    strides = [stride] + [1]*(num_blocks-1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "        layers.append(block(self.in_planes, planes, stride))\n",
        "        self.in_planes = planes * block.expansion\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.maxpool(out)\n",
        "\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "\n",
        "    out = F.avg_pool2d(out, out.size()[3])\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJmSe7bTdAjR",
        "colab_type": "text"
      },
      "source": [
        "##step 3 define resnt 18\n",
        "[2,2,2,2] blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrNLKNlUdH-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet18():\n",
        "  return ResNet(BasicBlock, [2,2,2,2])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BWxErBLRrDz",
        "colab_type": "text"
      },
      "source": [
        "#Setting up the train paramter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy4ljVi0R4k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "learnrate = 0.1\n",
        "log_interval = 1\n",
        "val_interval = 1\n",
        "start_epoch = -1\n",
        "milestones = [92, 136]  # divide it by 10 at 32k and 48k iterations\n",
        "model = resnet18()\n",
        "MAX_EPOCH = 182 # 64000 / (45000 / 128) = 182 epochs\n",
        "\n",
        "model.to(device)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learnrate, momentum=0.9, weight_decay=1e-4)  # é€‰æ‹©ä¼˜åŒ–å™¨\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, gamma=0.1, milestones=milestones)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgdBBmhIgb0i",
        "colab_type": "text"
      },
      "source": [
        "#Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBsuNB2gevJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf16218c-a693-48e2-dc2b-530fc9493efe"
      },
      "source": [
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
        "\n",
        "    # è®­ç»ƒ(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch)\n",
        "   \n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if train_on_gpu:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      # clear the gradients of all optimized variables\n",
        "      optimizer.zero_grad()\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model(data)\n",
        "      # calculate the batch loss\n",
        "      loss = criterion(output, target)\n",
        "      # backward pass: compute gradient of the loss with respect to model parameters\n",
        "      loss.backward()\n",
        "      # perform a single optimization step (parameter update)\n",
        "      optimizer.step()\n",
        "      # update training loss\n",
        "      train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "      # move tensors to GPU if CUDA is available\n",
        "      if train_on_gpu:\n",
        "          data, target = data.cuda(), target.cuda()\n",
        "      # forward pass: compute predicted outputs by passing inputs to the model\n",
        "      output = model(data)\n",
        "      # calculate the batch loss\n",
        "      loss = criterion(output, target)\n",
        "      # update average validation loss \n",
        "      valid_loss += loss.item()*data.size(0)\n",
        "  \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    scheduler.step()  # æ›´æ–°å­¦ä¹ çŽ‡\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if epoch > (MAX_EPOCH/2) and valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 \tTraining Loss: 1.790340 \tValidation Loss: 0.372924\n",
            "Epoch: 1 \tTraining Loss: 1.435311 \tValidation Loss: 0.350290\n",
            "Epoch: 2 \tTraining Loss: 1.333638 \tValidation Loss: 0.300823\n",
            "Epoch: 3 \tTraining Loss: 1.250097 \tValidation Loss: 0.303755\n",
            "Epoch: 4 \tTraining Loss: 1.172979 \tValidation Loss: 0.284557\n",
            "Epoch: 5 \tTraining Loss: 1.119744 \tValidation Loss: 0.265675\n",
            "Epoch: 6 \tTraining Loss: 1.075756 \tValidation Loss: 0.277517\n",
            "Epoch: 7 \tTraining Loss: 1.049081 \tValidation Loss: 0.251604\n",
            "Epoch: 8 \tTraining Loss: 1.007327 \tValidation Loss: 0.239689\n",
            "Epoch: 9 \tTraining Loss: 0.988614 \tValidation Loss: 0.236811\n",
            "Epoch: 10 \tTraining Loss: 0.963371 \tValidation Loss: 0.250571\n",
            "Epoch: 11 \tTraining Loss: 0.935467 \tValidation Loss: 0.237369\n",
            "Epoch: 12 \tTraining Loss: 0.921139 \tValidation Loss: 0.224917\n",
            "Epoch: 13 \tTraining Loss: 0.904150 \tValidation Loss: 0.235615\n",
            "Epoch: 14 \tTraining Loss: 0.889601 \tValidation Loss: 0.243917\n",
            "Epoch: 15 \tTraining Loss: 0.878744 \tValidation Loss: 0.201503\n",
            "Epoch: 16 \tTraining Loss: 0.869969 \tValidation Loss: 0.210219\n",
            "Epoch: 17 \tTraining Loss: 0.856953 \tValidation Loss: 0.229387\n",
            "Epoch: 18 \tTraining Loss: 0.848991 \tValidation Loss: 0.202813\n",
            "Epoch: 19 \tTraining Loss: 0.835018 \tValidation Loss: 0.183795\n",
            "Epoch: 20 \tTraining Loss: 0.834278 \tValidation Loss: 0.205983\n",
            "Epoch: 21 \tTraining Loss: 0.820913 \tValidation Loss: 0.191420\n",
            "Epoch: 22 \tTraining Loss: 0.815382 \tValidation Loss: 0.181151\n",
            "Epoch: 23 \tTraining Loss: 0.805020 \tValidation Loss: 0.207326\n",
            "Epoch: 24 \tTraining Loss: 0.809328 \tValidation Loss: 0.200518\n",
            "Epoch: 25 \tTraining Loss: 0.801272 \tValidation Loss: 0.198439\n",
            "Epoch: 26 \tTraining Loss: 0.786335 \tValidation Loss: 0.207895\n",
            "Epoch: 27 \tTraining Loss: 0.796965 \tValidation Loss: 0.202325\n",
            "Epoch: 28 \tTraining Loss: 0.784803 \tValidation Loss: 0.195955\n",
            "Epoch: 29 \tTraining Loss: 0.789517 \tValidation Loss: 0.199312\n",
            "Epoch: 30 \tTraining Loss: 0.784258 \tValidation Loss: 0.213708\n",
            "Epoch: 31 \tTraining Loss: 0.769222 \tValidation Loss: 0.210853\n",
            "Epoch: 32 \tTraining Loss: 0.770638 \tValidation Loss: 0.200553\n",
            "Epoch: 33 \tTraining Loss: 0.777520 \tValidation Loss: 0.198346\n",
            "Epoch: 34 \tTraining Loss: 0.767975 \tValidation Loss: 0.177865\n",
            "Epoch: 35 \tTraining Loss: 0.773396 \tValidation Loss: 0.188587\n",
            "Epoch: 36 \tTraining Loss: 0.761718 \tValidation Loss: 0.220981\n",
            "Epoch: 37 \tTraining Loss: 0.767573 \tValidation Loss: 0.192595\n",
            "Epoch: 38 \tTraining Loss: 0.759597 \tValidation Loss: 0.185147\n",
            "Epoch: 39 \tTraining Loss: 0.767948 \tValidation Loss: 0.183208\n",
            "Epoch: 40 \tTraining Loss: 0.761621 \tValidation Loss: 0.175653\n",
            "Epoch: 41 \tTraining Loss: 0.758617 \tValidation Loss: 0.198427\n",
            "Epoch: 42 \tTraining Loss: 0.753771 \tValidation Loss: 0.183989\n",
            "Epoch: 43 \tTraining Loss: 0.754837 \tValidation Loss: 0.190158\n",
            "Epoch: 44 \tTraining Loss: 0.754261 \tValidation Loss: 0.216473\n",
            "Epoch: 45 \tTraining Loss: 0.753270 \tValidation Loss: 0.194916\n",
            "Epoch: 46 \tTraining Loss: 0.746930 \tValidation Loss: 0.203071\n",
            "Epoch: 47 \tTraining Loss: 0.748785 \tValidation Loss: 0.189345\n",
            "Epoch: 48 \tTraining Loss: 0.751412 \tValidation Loss: 0.180675\n",
            "Epoch: 49 \tTraining Loss: 0.748670 \tValidation Loss: 0.234590\n",
            "Epoch: 50 \tTraining Loss: 0.748945 \tValidation Loss: 0.204260\n",
            "Epoch: 51 \tTraining Loss: 0.747875 \tValidation Loss: 0.230964\n",
            "Epoch: 52 \tTraining Loss: 0.738360 \tValidation Loss: 0.191117\n",
            "Epoch: 53 \tTraining Loss: 0.738482 \tValidation Loss: 0.221501\n",
            "Epoch: 54 \tTraining Loss: 0.749276 \tValidation Loss: 0.183967\n",
            "Epoch: 55 \tTraining Loss: 0.732518 \tValidation Loss: 0.182471\n",
            "Epoch: 56 \tTraining Loss: 0.732353 \tValidation Loss: 0.190093\n",
            "Epoch: 57 \tTraining Loss: 0.740559 \tValidation Loss: 0.183359\n",
            "Epoch: 58 \tTraining Loss: 0.736215 \tValidation Loss: 0.209847\n",
            "Epoch: 59 \tTraining Loss: 0.737269 \tValidation Loss: 0.174709\n",
            "Epoch: 60 \tTraining Loss: 0.739202 \tValidation Loss: 0.178852\n",
            "Epoch: 61 \tTraining Loss: 0.735474 \tValidation Loss: 0.191147\n",
            "Epoch: 62 \tTraining Loss: 0.738934 \tValidation Loss: 0.186068\n",
            "Epoch: 63 \tTraining Loss: 0.738664 \tValidation Loss: 0.199794\n",
            "Epoch: 64 \tTraining Loss: 0.730324 \tValidation Loss: 0.177948\n",
            "Epoch: 65 \tTraining Loss: 0.743019 \tValidation Loss: 0.180417\n",
            "Epoch: 66 \tTraining Loss: 0.732258 \tValidation Loss: 0.216727\n",
            "Epoch: 67 \tTraining Loss: 0.727299 \tValidation Loss: 0.177815\n",
            "Epoch: 68 \tTraining Loss: 0.736119 \tValidation Loss: 0.200095\n",
            "Epoch: 69 \tTraining Loss: 0.733654 \tValidation Loss: 0.181004\n",
            "Epoch: 70 \tTraining Loss: 0.726349 \tValidation Loss: 0.188493\n",
            "Epoch: 71 \tTraining Loss: 0.737119 \tValidation Loss: 0.166077\n",
            "Epoch: 72 \tTraining Loss: 0.727407 \tValidation Loss: 0.214427\n",
            "Epoch: 73 \tTraining Loss: 0.730799 \tValidation Loss: 0.197205\n",
            "Epoch: 74 \tTraining Loss: 0.729939 \tValidation Loss: 0.186457\n",
            "Epoch: 75 \tTraining Loss: 0.719230 \tValidation Loss: 0.188753\n",
            "Epoch: 76 \tTraining Loss: 0.735152 \tValidation Loss: 0.196176\n",
            "Epoch: 77 \tTraining Loss: 0.727989 \tValidation Loss: 0.202401\n",
            "Epoch: 78 \tTraining Loss: 0.729875 \tValidation Loss: 0.176207\n",
            "Epoch: 79 \tTraining Loss: 0.725244 \tValidation Loss: 0.175286\n",
            "Epoch: 80 \tTraining Loss: 0.733435 \tValidation Loss: 0.189143\n",
            "Epoch: 81 \tTraining Loss: 0.719236 \tValidation Loss: 0.191186\n",
            "Epoch: 82 \tTraining Loss: 0.728521 \tValidation Loss: 0.188821\n",
            "Epoch: 83 \tTraining Loss: 0.722072 \tValidation Loss: 0.195572\n",
            "Epoch: 84 \tTraining Loss: 0.722821 \tValidation Loss: 0.221685\n",
            "Epoch: 85 \tTraining Loss: 0.731242 \tValidation Loss: 0.162774\n",
            "Epoch: 86 \tTraining Loss: 0.725418 \tValidation Loss: 0.174567\n",
            "Epoch: 87 \tTraining Loss: 0.726110 \tValidation Loss: 0.198505\n",
            "Epoch: 88 \tTraining Loss: 0.727310 \tValidation Loss: 0.191662\n",
            "Epoch: 89 \tTraining Loss: 0.728904 \tValidation Loss: 0.187087\n",
            "Epoch: 90 \tTraining Loss: 0.721735 \tValidation Loss: 0.190934\n",
            "Epoch: 91 \tTraining Loss: 0.726556 \tValidation Loss: 0.180094\n",
            "Epoch: 92 \tTraining Loss: 0.492620 \tValidation Loss: 0.120583\n",
            "Validation loss decreased (inf --> 0.120583).  Saving model ...\n",
            "Epoch: 93 \tTraining Loss: 0.440777 \tValidation Loss: 0.114700\n",
            "Validation loss decreased (0.120583 --> 0.114700).  Saving model ...\n",
            "Epoch: 94 \tTraining Loss: 0.420069 \tValidation Loss: 0.114687\n",
            "Validation loss decreased (0.114700 --> 0.114687).  Saving model ...\n",
            "Epoch: 95 \tTraining Loss: 0.402606 \tValidation Loss: 0.110702\n",
            "Validation loss decreased (0.114687 --> 0.110702).  Saving model ...\n",
            "Epoch: 96 \tTraining Loss: 0.385155 \tValidation Loss: 0.108695\n",
            "Validation loss decreased (0.110702 --> 0.108695).  Saving model ...\n",
            "Epoch: 97 \tTraining Loss: 0.376444 \tValidation Loss: 0.107072\n",
            "Validation loss decreased (0.108695 --> 0.107072).  Saving model ...\n",
            "Epoch: 98 \tTraining Loss: 0.364858 \tValidation Loss: 0.108001\n",
            "Epoch: 99 \tTraining Loss: 0.358797 \tValidation Loss: 0.108340\n",
            "Epoch: 100 \tTraining Loss: 0.351195 \tValidation Loss: 0.105012\n",
            "Validation loss decreased (0.107072 --> 0.105012).  Saving model ...\n",
            "Epoch: 101 \tTraining Loss: 0.348414 \tValidation Loss: 0.104129\n",
            "Validation loss decreased (0.105012 --> 0.104129).  Saving model ...\n",
            "Epoch: 102 \tTraining Loss: 0.340303 \tValidation Loss: 0.102760\n",
            "Validation loss decreased (0.104129 --> 0.102760).  Saving model ...\n",
            "Epoch: 103 \tTraining Loss: 0.333895 \tValidation Loss: 0.106158\n",
            "Epoch: 104 \tTraining Loss: 0.329358 \tValidation Loss: 0.104773\n",
            "Epoch: 105 \tTraining Loss: 0.322244 \tValidation Loss: 0.104743\n",
            "Epoch: 106 \tTraining Loss: 0.321337 \tValidation Loss: 0.104496\n",
            "Epoch: 107 \tTraining Loss: 0.313781 \tValidation Loss: 0.102537\n",
            "Validation loss decreased (0.102760 --> 0.102537).  Saving model ...\n",
            "Epoch: 108 \tTraining Loss: 0.313665 \tValidation Loss: 0.103185\n",
            "Epoch: 109 \tTraining Loss: 0.310038 \tValidation Loss: 0.105888\n",
            "Epoch: 110 \tTraining Loss: 0.312760 \tValidation Loss: 0.108452\n",
            "Epoch: 111 \tTraining Loss: 0.301654 \tValidation Loss: 0.102066\n",
            "Validation loss decreased (0.102537 --> 0.102066).  Saving model ...\n",
            "Epoch: 112 \tTraining Loss: 0.301786 \tValidation Loss: 0.106781\n",
            "Epoch: 113 \tTraining Loss: 0.303926 \tValidation Loss: 0.104804\n",
            "Epoch: 114 \tTraining Loss: 0.298281 \tValidation Loss: 0.101926\n",
            "Validation loss decreased (0.102066 --> 0.101926).  Saving model ...\n",
            "Epoch: 115 \tTraining Loss: 0.299087 \tValidation Loss: 0.110102\n",
            "Epoch: 116 \tTraining Loss: 0.293533 \tValidation Loss: 0.107180\n",
            "Epoch: 117 \tTraining Loss: 0.298217 \tValidation Loss: 0.108499\n",
            "Epoch: 118 \tTraining Loss: 0.293405 \tValidation Loss: 0.107123\n",
            "Epoch: 119 \tTraining Loss: 0.295549 \tValidation Loss: 0.106381\n",
            "Epoch: 120 \tTraining Loss: 0.290560 \tValidation Loss: 0.106467\n",
            "Epoch: 121 \tTraining Loss: 0.288847 \tValidation Loss: 0.108218\n",
            "Epoch: 122 \tTraining Loss: 0.285906 \tValidation Loss: 0.105912\n",
            "Epoch: 123 \tTraining Loss: 0.288774 \tValidation Loss: 0.104095\n",
            "Epoch: 124 \tTraining Loss: 0.285900 \tValidation Loss: 0.111283\n",
            "Epoch: 125 \tTraining Loss: 0.284356 \tValidation Loss: 0.110130\n",
            "Epoch: 126 \tTraining Loss: 0.286135 \tValidation Loss: 0.108370\n",
            "Epoch: 127 \tTraining Loss: 0.285055 \tValidation Loss: 0.110786\n",
            "Epoch: 128 \tTraining Loss: 0.282589 \tValidation Loss: 0.112410\n",
            "Epoch: 129 \tTraining Loss: 0.280160 \tValidation Loss: 0.104605\n",
            "Epoch: 130 \tTraining Loss: 0.278890 \tValidation Loss: 0.114431\n",
            "Epoch: 131 \tTraining Loss: 0.279129 \tValidation Loss: 0.110603\n",
            "Epoch: 132 \tTraining Loss: 0.283985 \tValidation Loss: 0.107499\n",
            "Epoch: 133 \tTraining Loss: 0.282428 \tValidation Loss: 0.105507\n",
            "Epoch: 134 \tTraining Loss: 0.275847 \tValidation Loss: 0.110260\n",
            "Epoch: 135 \tTraining Loss: 0.278713 \tValidation Loss: 0.104093\n",
            "Epoch: 136 \tTraining Loss: 0.213096 \tValidation Loss: 0.094260\n",
            "Validation loss decreased (0.101926 --> 0.094260).  Saving model ...\n",
            "Epoch: 137 \tTraining Loss: 0.186372 \tValidation Loss: 0.094833\n",
            "Epoch: 138 \tTraining Loss: 0.176905 \tValidation Loss: 0.097056\n",
            "Epoch: 139 \tTraining Loss: 0.171448 \tValidation Loss: 0.096931\n",
            "Epoch: 140 \tTraining Loss: 0.165522 \tValidation Loss: 0.094755\n",
            "Epoch: 141 \tTraining Loss: 0.162379 \tValidation Loss: 0.096428\n",
            "Epoch: 142 \tTraining Loss: 0.159909 \tValidation Loss: 0.093762\n",
            "Validation loss decreased (0.094260 --> 0.093762).  Saving model ...\n",
            "Epoch: 143 \tTraining Loss: 0.153443 \tValidation Loss: 0.096099\n",
            "Epoch: 144 \tTraining Loss: 0.152445 \tValidation Loss: 0.097887\n",
            "Epoch: 145 \tTraining Loss: 0.150773 \tValidation Loss: 0.095384\n",
            "Epoch: 146 \tTraining Loss: 0.148533 \tValidation Loss: 0.094698\n",
            "Epoch: 147 \tTraining Loss: 0.141232 \tValidation Loss: 0.098381\n",
            "Epoch: 148 \tTraining Loss: 0.144930 \tValidation Loss: 0.099641\n",
            "Epoch: 149 \tTraining Loss: 0.140777 \tValidation Loss: 0.097128\n",
            "Epoch: 150 \tTraining Loss: 0.138108 \tValidation Loss: 0.098310\n",
            "Epoch: 151 \tTraining Loss: 0.137974 \tValidation Loss: 0.096524\n",
            "Epoch: 152 \tTraining Loss: 0.136337 \tValidation Loss: 0.097196\n",
            "Epoch: 153 \tTraining Loss: 0.133352 \tValidation Loss: 0.099190\n",
            "Epoch: 154 \tTraining Loss: 0.130610 \tValidation Loss: 0.099762\n",
            "Epoch: 155 \tTraining Loss: 0.129140 \tValidation Loss: 0.099465\n",
            "Epoch: 156 \tTraining Loss: 0.130152 \tValidation Loss: 0.100982\n",
            "Epoch: 157 \tTraining Loss: 0.126575 \tValidation Loss: 0.097841\n",
            "Epoch: 158 \tTraining Loss: 0.126145 \tValidation Loss: 0.104302\n",
            "Epoch: 159 \tTraining Loss: 0.124711 \tValidation Loss: 0.101055\n",
            "Epoch: 160 \tTraining Loss: 0.122829 \tValidation Loss: 0.104654\n",
            "Epoch: 161 \tTraining Loss: 0.122032 \tValidation Loss: 0.098104\n",
            "Epoch: 162 \tTraining Loss: 0.120968 \tValidation Loss: 0.102371\n",
            "Epoch: 163 \tTraining Loss: 0.117614 \tValidation Loss: 0.102338\n",
            "Epoch: 164 \tTraining Loss: 0.115718 \tValidation Loss: 0.100318\n",
            "Epoch: 165 \tTraining Loss: 0.113883 \tValidation Loss: 0.105733\n",
            "Epoch: 166 \tTraining Loss: 0.115051 \tValidation Loss: 0.104837\n",
            "Epoch: 167 \tTraining Loss: 0.116274 \tValidation Loss: 0.102020\n",
            "Epoch: 168 \tTraining Loss: 0.111088 \tValidation Loss: 0.105357\n",
            "Epoch: 169 \tTraining Loss: 0.109959 \tValidation Loss: 0.104772\n",
            "Epoch: 170 \tTraining Loss: 0.109809 \tValidation Loss: 0.104611\n",
            "Epoch: 171 \tTraining Loss: 0.111384 \tValidation Loss: 0.106024\n",
            "Epoch: 172 \tTraining Loss: 0.107647 \tValidation Loss: 0.106405\n",
            "Epoch: 173 \tTraining Loss: 0.106127 \tValidation Loss: 0.106415\n",
            "Epoch: 174 \tTraining Loss: 0.106673 \tValidation Loss: 0.107482\n",
            "Epoch: 175 \tTraining Loss: 0.105320 \tValidation Loss: 0.106114\n",
            "Epoch: 176 \tTraining Loss: 0.100830 \tValidation Loss: 0.108368\n",
            "Epoch: 177 \tTraining Loss: 0.103299 \tValidation Loss: 0.104777\n",
            "Epoch: 178 \tTraining Loss: 0.102003 \tValidation Loss: 0.110135\n",
            "Epoch: 179 \tTraining Loss: 0.097876 \tValidation Loss: 0.105289\n",
            "Epoch: 180 \tTraining Loss: 0.100675 \tValidation Loss: 0.110540\n",
            "Epoch: 181 \tTraining Loss: 0.098859 \tValidation Loss: 0.106305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWI2MCXAr8MF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "fa9bc2ad-6038-4b54-b05c-d2f920239492"
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.535036\n",
            "\n",
            "Test Accuracy of airplane: 85% (858/1000)\n",
            "Test Accuracy of automobile: 94% (940/1000)\n",
            "Test Accuracy of  bird: 82% (827/1000)\n",
            "Test Accuracy of   cat: 73% (734/1000)\n",
            "Test Accuracy of  deer: 84% (841/1000)\n",
            "Test Accuracy of   dog: 76% (767/1000)\n",
            "Test Accuracy of  frog: 89% (898/1000)\n",
            "Test Accuracy of horse: 90% (903/1000)\n",
            "Test Accuracy of  ship: 92% (925/1000)\n",
            "Test Accuracy of truck: 90% (907/1000)\n",
            "\n",
            "Test Accuracy (Overall): 86% (8600/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}